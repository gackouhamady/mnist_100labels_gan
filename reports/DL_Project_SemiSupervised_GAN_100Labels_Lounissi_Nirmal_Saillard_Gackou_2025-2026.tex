\documentclass{article}
\usepackage[preprint]{neurips_2024}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath, amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}

% ============================================================
% TITLE + AUTHORS
% ============================================================

\title{Semi-Supervised GANs for MNIST Classification with 100 Labels:\\
An Experimental Study of the K+1 Discriminator and Feature Matching}

\author{
  Lounissi \\
  Université Paris Cité \\
  \texttt{firstname.lastname@example.com}
  \And
  Nirmal \\
  Université Paris Cité \\
  \texttt{firstname.lastname@example.com}
  \And
  Saillard \\
  Université Paris Cité \\
  \texttt{firstname.lastname@example.com}
  \And
  Hamady Gackou \\
  Université Paris Cité \\
  \texttt{hamady.gackou@example.com}
}

\begin{document}

\maketitle

% ============================================================
% ABSTRACT
% ============================================================

\begin{abstract}
We investigate semi-supervised classification on the MNIST dataset using a severely limited labeled subset of only 100 samples (10 per class). We compare a standard supervised Convolutional Neural Network (CNN) baseline against a Semi-Supervised Generative Adversarial Network (SGAN) framework. Our SGAN implementation leverages a $(K+1)$-class discriminator to learn from unlabeled data and utilizes feature matching to stabilize generator training. Experimental results demonstrate a significant performance gap: while the supervised baseline achieves a test accuracy of 82.73\%, our SGAN model reaches 97.82\%, effectively utilizing the structure of unlabeled data to generalize well in the low-label regime.
\end{abstract}

% ============================================================
% 1. INTRODUCTION
% ============================================================

\section{Introduction}

Deep learning models typically require vast amounts of labeled data to achieve state-of-the-art performance. However, in many real-world applications, obtaining high-quality labels is expensive and time-consuming, while unlabeled data is often abundant. This necessitates the development of semi-supervised learning algorithms that can leverage unlabeled data to improve generalization.

Generative Adversarial Networks (GANs), introduced by Goodfellow et al. [1], have shown remarkable success in generating realistic data. Beyond generation, Salimans et al. [2] proposed techniques to adapt GANs for semi-supervised learning. By extending the discriminator to classify samples into $K$ real classes or a $(K+1)$-th "fake" class, the model can learn from both labeled and unlabeled data simultaneously.

In this work, we address the problem of classifying MNIST digits using only 100 labeled examples (balanced across classes). We implement and evaluate an SGAN with feature matching loss, comparing it against a supervised CNN baseline. Our primary contribution is an experimental validation showing that the SGAN framework effectively prevents overfitting and learns robust representations, outperforming the baseline by over 15 percentage points.

% ============================================================
% 2. BASELINE METHOD
% ============================================================

\section{Baseline Method}

To establish a lower bound on performance, we trained a supervised Convolutional Neural Network (CNN) using only the 100 labeled samples.

\subsection{Architecture}
The baseline model consists of two convolutional blocks followed by a fully connected classifier:
\begin{itemize}
    \item \textbf{Features}: Conv2d(1, 32) $\to$ ReLU $\to$ MaxPool $\to$ Conv2d(32, 64) $\to$ ReLU $\to$ MaxPool.
    \item \textbf{Classifier}: Flatten $\to$ Linear(3136, 128) $\to$ ReLU $\to$ Dropout(0.3) $\to$ Linear(128, 10).
\end{itemize}

\subsection{Training Setup}
The model was trained using the Cross-Entropy loss and the Adam optimizer with a learning rate of $1 \times 10^{-3}$ and weight decay of $1 \times 10^{-4}$. We employed early stopping with a patience of 10 epochs to mitigate the severe overfitting caused by the small training set size. The training was conducted for a maximum of 50 epochs.

% ============================================================
% 3. SGAN METHODOLOGY
% ============================================================

\section{SGAN Methodology}

Our Semi-Supervised GAN extends the standard GAN framework by modifying the discriminator's objective and stabilizing the generator via feature matching.

\subsection{K+1 Classifier}

The discriminator $D$ is a classifier with $K+1$ output units, where $K=10$ corresponds to the digit classes and the $(K+1)$-th unit corresponds to generated ("fake") images. The probability of a sample $x$ belonging to class $k$ is given by the softmax over $K+1$ outputs. The loss function for $D$ is composed of two parts:

1.  \textbf{Supervised Loss ($\mathcal{L}_{sup}$)}: Trained on the labeled data $\mathcal{D}_L$. The discriminator must assign the correct label $y \in \{1, \dots, K\}$ to the real sample $x$.
    \[
    \mathcal{L}_{sup} = - \mathbb{E}_{(x,y) \sim \mathcal{D}_L} \log p_{model}(y|x, y < K+1)
    \]
2.  \textbf{Unsupervised Loss ($\mathcal{L}_{unsup}$)}: Trained on unlabeled data $\mathcal{D}_U$ and generated data $G(z)$. The discriminator must classify real unlabeled samples as belonging to *any* of the first $K$ classes (real), and generated samples as the $(K+1)$-th class (fake).
    \[
    \mathcal{L}_{unsup} = - \left\{ \mathbb{E}_{x \sim \mathcal{D}_U} \log [1 - p(y=K+1|x)] + \mathbb{E}_{z \sim p(z)} \log p(y=K+1|G(z)) \right\}
    \]

The total discriminator loss is $\mathcal{L}_D = \mathcal{L}_{sup} + \lambda \mathcal{L}_{unsup}$, where $\lambda=1.0$ in our experiments.

\subsection{Feature Matching}

To improve stability and effectively use the discriminator's learned features, we train the generator using \textit{Feature Matching} [2]. Instead of maximizing the discriminator's error directly, the generator minimizes the $L_2$ distance between the expected features of real data and generated data at an intermediate layer of the discriminator.
\[
\mathcal{L}_G = \left\| \mathbb{E}_{x \sim \mathcal{D}_U} f(x) - \mathbb{E}_{z \sim p(z)} f(G(z)) \right\|_2^2
\]
where $f(x)$ denotes the activations from the intermediate layer of the discriminator.

% ============================================================
% 4. IMPLEMENTATION DETAILS
% ============================================================

\section{Implementation Details}

\textbf{Dataset Preparation}: We utilized the standard MNIST dataset. A balanced subset of 100 samples (10 per class) was selected as the labeled set $\mathcal{D}_L$. The remaining 59,900 training samples were treated as the unlabeled set $\mathcal{D}_U$. The standard 10,000 test samples were used for evaluation.

\textbf{Architectures}:
\begin{itemize}
    \item \textbf{Generator}: A DCGAN-style generator taking a noise vector $z \in \mathbb{R}^{100}$, projecting it via a linear layer, and upsampling via transposed convolutions to a $28 \times 28$ image.
    \item \textbf{Discriminator}: A CNN with LeakyReLU activations and Dropout, outputting logits of dimension 11.
\end{itemize}

\textbf{Hyperparameters}:
\begin{itemize}
    \item Optimization: Adam optimizer with $\beta_1=0.5, \beta_2=0.999$.
    \item Learning Rates: $2 \times 10^{-4}$ for both $G$ and $D$.
    \item Batch Sizes: 64 for labeled data, 128 for unlabeled data.
    \item Training Duration: Maximum 100 epochs with early stopping (patience=15).
    \item Hardware: NVIDIA L4 GPU.
    \item Random Seed: Fixed to 42 for reproducibility.
\end{itemize}

% ============================================================
% 5. EXPERIMENTS AND RESULTS
% ============================================================

\section{Experiments and Results}

We evaluated both models based on classification accuracy on the held-out test set. The results are summarized in Table 1.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Method & Best Test Accuracy (\%) & Best Epoch \\
\midrule
Baseline CNN (Supervised) & 82.73\% & 49 \\
\textbf{SGAN (K+1 + Feature Matching)} & \textbf{97.82\%} & 62 \\
\bottomrule
\end{tabular}
\caption{Comparison of classification accuracy on the MNIST test set using only 100 labeled training samples.}
\label{tab:results}
\end{table}

The Baseline CNN suffered significantly from the lack of data, plateauing around 82\% accuracy. In contrast, the SGAN effectively utilized the unlabeled data. By epoch 10, the SGAN had already surpassed 95\% accuracy. The training was stable, with the generator loss remaining bounded due to feature matching. The final model achieved near fully-supervised performance levels (approx. 98\%), demonstrating the efficacy of the semi-supervised approach.

% ============================================================
% 6. DISCUSSION
% ============================================================

\section{Discussion}

The substantial improvement of the SGAN over the baseline (approx. +15\%) confirms that the structural information contained in the unlabeled data is crucial for generalization in low-data regimes. The $K+1$ discriminator formulation forces the model to learn decision boundaries that not only separate the 10 labeled classes but also distinguish the manifold of real digit images from the generated noise.

Feature matching played a critical role in stabilizing the adversarial game. Unlike standard Minimax GAN training, which often suffers from vanishing gradients or mode collapse, feature matching provided a consistent learning signal for the generator, ensuring the discriminator learned robust features useful for classification.

% ============================================================
% 7. CONCLUSION
% ============================================================

\section{Conclusion}

We successfully implemented a Semi-Supervised GAN to address image classification with extreme label scarcity. By training on only 100 labeled MNIST samples, our SGAN model achieved 97.82\% accuracy, significantly outperforming a standard supervised baseline. This highlights the potential of adversarial generative models to act as powerful semi-supervised learners by exploiting abundant unlabeled data. Future work could extend this approach to more complex datasets like CIFAR-10 or SVHN using deeper architectures such as ResNets.

% ============================================================
% 8. APPENDIX
% ============================================================

\appendix

\section*{Appendix: Code Excerpts}

\textbf{Discriminator Loss Calculation}:
\begin{verbatim}
# Labeled data loss (Supervised)
logits_l = D(x_l)[:, :10] # First 10 classes
loss_sup = ce_loss(logits_l, y_l)

# Unlabeled data loss (Real vs Fake)
logits_u = D(x_u)
# Prob that unlabeled data is fake (class 10)
p_fake_u = torch.softmax(logits_u, dim=1)[:, 10] 
loss_unsup_real = -torch.log(1.0 - p_fake_u + EPS).mean()

# Generated data loss (Fake should be class 10)
logits_fake = D(x_fake)
p_fake_g = torch.softmax(logits_fake, dim=1)[:, 10]
loss_unsup_fake = -torch.log(p_fake_g + EPS).mean()

total_loss_D = loss_sup + LAMBDA * (loss_unsup_real + loss_unsup_fake)
\end{verbatim}

\textbf{Feature Matching Loss}:
\begin{verbatim}
# Extract intermediate features from D
_, feat_fake = D(x_fake, return_features=True)
_, feat_real = D(x_u, return_features=True)

# Minimize L2 distance between feature statistics
loss_G = (feat_real.mean(0) - feat_fake.mean(0)).pow(2).mean()
\end{verbatim}

% ============================================================
% REFERENCES
% ============================================================

\medskip

\begin{thebibliography}{9}

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pages 2672--2680, 2014.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Alec Radford, Vicki Cheung, and Xi Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in neural information processing systems}, pages 2234--2242, 2016.

\bibitem{lecun1998gradient}
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{kingma2014adam}
Diederik P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}

\end{document}