# # Semi-Supervised GAN for MNIST (100 Labels)

<p align="center">
<img alt="University Paris CitÃ©" src="https://img.shields.io/badge/University-Paris%20CitÃ©-6f42c1?style=for-the-badge&logo=academia&logoColor=white">
<img alt="Master ML for Data Science" src="https://img.shields.io/badge/Master-Machine%20Learning%20for%20Data%20Science-1976D2?style=for-the-badge&logo=python&logoColor=white">
<img alt="Deep Learning Project" src="https://img.shields.io/badge/Project-Deep%20Learning%20-%20Semi--Supervised%20GAN-FF9800?style=for-the-badge&logo=jupyter&logoColor=white">
<img alt="Academic Year" src="https://img.shields.io/badge/Year-2025%2F2026-009688?style=for-the-badge&logo=googlecalendar&logoColor=white">
</p>

---

## Project Team

**UniversitÃ© Paris CitÃ© â€” Master 2 Machine Learning for Data Science**

* **Manel LOUNISSI** ([manel2.lounissi@gmail.com](mailto:manel2.lounissi@gmail.com))
* **Sandeep-Singh NIRMAL** ([nirmalsinghsandeep@gmail.com](mailto:nirmalsinghsandeep@gmail.com))
* **Brice SAILLARD** ([brice.saillard.bs@gmail.com](mailto:brice.saillard.bs@gmail.com))
* **Hamady GACKOU** ([hamady.gackou@etu.u-paris.fr](mailto:hamady.gackou@etu.u-paris.fr))

**Supervisor:** Blaise Hanczar

---

##  Project Summary

This project explores the power of **semi-supervised learning** using Generative Adversarial Networks (GAN). In a scenario where only **100 labeled images** (10 per class) are available out of the 60,000 in the MNIST dataset, we demonstrate how a **Semi-Supervised GAN (SGAN)** can drastically outperform a classic CNN.

### The Solution: Discriminator

The core of our approach lies in modifying the discriminator so that it does not just distinguish "real" from "fake," but acts as an 11-class classifier:

* **Classes 0-9:** Real handwritten digits.
* **Class 10:** Generated images ("Fake").

---

##  Comparative Performance

| Model | Labeled Data | Unlabeled Data | Test Accuracy (%) |
| --- | --- | --- | --- |
| **Baseline CNN** | 100 | None | 82.73% |
| **SGAN (K+1 + Feature Matching)** | 100 | **59,900** | **97.82%** |

---

## ðŸ›  Code Structure & Pipeline

```bash
mnist_100labels_gan/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cnn_baseline.py       # Baseline model architecture
â”‚   â”œâ”€â”€ gan_generator.py      # DCGAN-style Generator
â”‚   â””â”€â”€ gan_discriminator.py  # Discriminator (K+1 logits)
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_baseline.py     # Supervised training script
â”‚   â””â”€â”€ train_semisup_gan.py  # SGAN logic + Feature Matching
â”œâ”€â”€ report/
â”‚   â””â”€â”€ report_neurips.pdf    # Final scientific report
â””â”€â”€ main.py                   # Single entry point

```

---

##  Report Plan (Scientific Structure)

Below is the rigorous plan adopted for the writing of our article (NeurIPS format):

1. **Introduction**
* The problem of labeling costs.
* Motivation for using GANs in semi-supervised learning.


2. **State of the Art & Baseline**
* Description of the supervised CNN.
* Analysis of overfitting in low-data regimes.


3. **SGAN Methodology**
*  classifier architecture.
* Formulation of loss functions (Supervised vs. Unsupervised).
* **Feature Matching:** Technique for stabilizing Generator training.


4. **Implementation Details**
* Hyperparameters (Adam, learning rates, batch sizes).
* MNIST dataset management (100/59,900 split).


5. **Experimental Results**
* Convergence and accuracy curves.
* Visualization of images generated by the SGAN.


6. **Discussion & Analysis**
* Why does the SGAN generalize better?
* The role of structural information from unlabeled data.


7. **Conclusion & Perspectives**
* Extensibility to more complex datasets (CIFAR-10).


8. **References & Appendices**

---

##  How to Reproduce

1. Clone the repository.
2. Run the full training:
```bash
python main.py
```
3. View results in `/experiments/results.json`.
---

<p align="center"><i>Produced with rigor and passion by the GACKOU-LOUNISSI-NIRMAL-SAILLARD team.</i></p>

---
